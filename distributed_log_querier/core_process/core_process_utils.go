package distributed_log_querier

import (
	"crypto/sha1"
	"fmt"
	"io"
	"math/big"
	"math/rand"
	"net"
	"os"
	"path/filepath"
	"runtime"
	"sort"
	"strconv"
	"strings"
	"sync"
	"sync/atomic"
	"time"
	"os/exec"
	"bytes"
	"encoding/json"
	"bufio"
	"slices"
	"regexp"
)


type Task struct{
	TaskID int
	TaskName string // optional : autogenerated for easier hydfsfile naming  Task+taskID
	TaskLogFile string // autogenerated as TaskName + .txt
	TaskType string //source or operator
	TaskStatus string // active or completed or dead
	TaskStage string // 1 or 2 or 3
	TaskInputType string // file or stream ( multi or single )
	TaskOutputType string // file or stream (multi (hash) or single)
	TaskOutputNodes []string // empty or NodeID/s 
	TaskOutputFile string // None or FileName (especially for key aggregators)
	TaskInputFile string // None or FileName
	TaskPartitionRange string //for source tasks only. Format x:y
	TaskInputNode string // None or NodeID (just to catch any stray relays)
	TaskState string // stateful or stateless 
	TaskOperatorName string // if type is operator
	TaskOperatorInput []string // "None" if no input else the input
	TaskReceiveAck bool // if the task has received an ack from the output node
	DoReplay bool // if the task needs to be replayed
	TaskAssignedNode int // node id of the node the task is assigned to
	OutputTaskIDs []int // task id of the task where the output needs to be delivered -1 if not applicable
	InputTaskID int // task id of the task where the input is coming from empty if not applicable
	NumTasks int // only for stage two to recognize when to stop (i.e. after collecting all the completes from previous stages) -1 if not applicable
}

//crashes the VM set from the leader, this includes till the failure detection layer, the hydfs layer and also the stream ds layer
//this is a hard crash and the VM will not be able to recover
//this is used for testing purposes only during demo
func killStreamDSProcess(nodeId int, hydfsConn *SafeConn, streamConnTable *sync.Map){
	//get the connection from the streamConnTable
	conn, ok := streamConnTable.Load(nodeId)
	if !ok{
		fmt.Println("Connection not found in the streamConnTable")
		return
	}
	//send the kill signal
	_, err := conn.(net.Conn).Write([]byte("KILLSTREAMDS: END_OF_MSG\n"))
	if err != nil{
		fmt.Println("Error sending kill signal to node", nodeId)
	}
	//now kill the hydfs layer as well. 
	hydfsConn.SafeWrite([]byte("KILLSTREAMDS: " + strconv.Itoa(nodeId) + "END_OF_MSG\n"))
	//then also kill the failure detection layer
	//need to use UDP message for this
	conn_remote_address := conn.(net.Conn).RemoteAddr().String() 
	fmt.Println("conn remote address ", conn_remote_address)
	conn_remote_address_pure := strings.Split(conn_remote_address, ":")[0]
	conn_remote_address_port := strings.Split(conn_remote_address, ":")[1]
	//subtract 1000 from the port to get the failure detection layer port
	//VM MARKER BEGIN
	fd_port := subtractStrings(conn_remote_address_port, 1090)
	//VM MARKER END
	//VM LOCAL MARKER
	//fd_port := subtractStrings(conn_remote_address_port, 1010)
	//VM LOCAL MARKER END
	fd_address := conn_remote_address_pure + ":" + fd_port
	fmt.Println("fd address ", fd_address)
	addr, err := net.ResolveUDPAddr("udp", fd_address)
	if err != nil {
		fmt.Println("Error resolving UDP address to kill Failure detection layer")
		return
	}
	communicateUDPToPeer("KILLSTREAMDS:",addr)
	fmt.Println("Kill message sent to the VM" )

}


func sendTask(conn net.Conn, task Task) error {
	data, err := SerializeTask(task)
	if err != nil {
		return err
	}
	// Add header and footer
	data = append([]byte("TASK:+: "), data...)
	data = append(data, []byte("END_OF_MSG\n")...)
	_, err = conn.Write(data)
	return err
}

func SerializeTask(task Task) ([]byte, error) {
    return json.Marshal(task)
}

func DeserializeTask(data []byte) (Task, error) {
    var task Task
    err := json.Unmarshal(data, &task)
    return task, err
}

const (
	bufferStart = "--- BUFFER START ---"
	bufferEnd   = "--- BUFFER END ---"
	timeFormat  = "2006-01-02 15:04:05"
	// Using a larger chunk size for better performance when reading large files
	readChunkSize = 4096
)
type LineInfo struct {
    FileLineID string
    Content    string
}


func ExecuteMergeOnHydfs(hydfsConn *SafeConn, filename string) {
	//call merge on file
	hydfsConn.SafeWrite([]byte("TASKMERGE: " + filename + "END_OF_MSG\n"))
}

func ResolveStoredAcks(ret_ack_map *map[string][]LineInfo, streamConnTable *sync.Map){
	//iterate through the map and send the acks to the respective nodes
	for nodeID_task_pair, acks := range *ret_ack_map{
		//get the connection from the streamConnTable
		nodeID := strings.Split(nodeID_task_pair, "-")[0]
		taskID := strings.Split(nodeID_task_pair, "-")[1]
		nodeID_int, err := strconv.Atoi(nodeID)
		if err != nil{
			fmt.Println("Error converting nodeID to int")
			continue
		}
		conn, ok := streamConnTable.Load(nodeID_int)
		if !ok{
			fmt.Println("Connection not found in the streamConnTable")
			continue
		}
		//fmt.Println("Sending acks to node : ", nodeID, len(acks))
		//send the acks
		err2 := sendAckInfoArray(conn.(net.Conn), acks, nodeID, taskID)
		if err2 != nil{
			fmt.Println("Error sending acks to node", nodeID)
		}
		
	}
	//clear the map
	*ret_ack_map = make(map[string][]LineInfo)
}

func QuantifyHydfsFile(hydfsConn *SafeConn,hydfsDestFileName string, n int) ([]string, error) {
	//first execute merge on the file
	ExecuteMergeOnHydfs(hydfsConn, hydfsDestFileName)
	//wait for 6 seconds to ensure the merge was successful
	time.Sleep(6 * time.Second)
	//then quantify the file
	GetFileFromHydfs(hydfsConn, hydfsDestFileName, 10)
	//wait for 1 second
	time.Sleep(1 * time.Second)
	// read last n buffers with distinct taskIDs
	output,error := GetLastNDistinctTaskBuffers(hydfsDestFileName, n)
	return output, error
}

func GetLastNDistinctTaskBuffers(fileName string, n int) ([]string, error) {
    filePath := filepath.Join(GetDistributedLogQuerierDir(), "Fetched", fileName)
    file, err := os.Open(filePath)
    if err != nil {
        return nil, err
    }
    defer file.Close()

    taskBuffers := make(map[int]string)
    var currentBuffer strings.Builder
    scanner := bufio.NewScanner(file)
    readingBuffer := false

    for scanner.Scan() {
        line := scanner.Text()
        if line == "--- BUFFER START ---" {
            readingBuffer = true
            currentBuffer.Reset()
            currentBuffer.WriteString(line + "\n") // Include buffer start marker
        } else if line == "--- BUFFER END ---" {
            if readingBuffer {
                currentBuffer.WriteString(line + "\n") // Include buffer end marker
                // Extract TaskID from the buffer content
                content := currentBuffer.String()
                taskIDMatch := regexp.MustCompile(`TaskID: (\d+)`).FindStringSubmatch(content)
                if len(taskIDMatch) > 1 {
                    taskID, _ := strconv.Atoi(taskIDMatch[1])
                    taskBuffers[taskID] = content
                }
            }
            readingBuffer = false
        } else if readingBuffer {
            currentBuffer.WriteString(line + "\n")
        }
    }

    if err := scanner.Err(); err != nil {
        return nil, err
    }

    // Get sorted task IDs to ensure consistent ordering
    var taskIDs []int
    for id := range taskBuffers {
        taskIDs = append(taskIDs, id)
    }
    sort.Sort(sort.Reverse(sort.IntSlice(taskIDs)))

    // Get last n distinct buffers
    lastNBuffers := make([]string, 0, n)
    for _, id := range taskIDs {
        if len(lastNBuffers) < n {
            lastNBuffers = append(lastNBuffers, taskBuffers[id])
        } else {
            break
        }
    }

    return lastNBuffers, nil
}




// func GetLastNDistinctTaskBuffers(fileName string, n int) ([]string, error) {
// 	filePath := filepath.Join(GetDistributedLogQuerierDir(), "Fetched", fileName)
//     file, err := os.Open(filePath)
//     if err != nil {
//         return nil, err
//     }
//     defer file.Close()

//     taskBuffers := make(map[int]string)
//     var currentBuffer strings.Builder
//     scanner := bufio.NewScanner(file)
//     readingBuffer := false
//     var currentTaskID int

//     for scanner.Scan() {
//         line := scanner.Text()
//         if line == "--- BUFFER START ---" {
//             readingBuffer = true
//             currentBuffer.Reset()
//         } else if line == "--- BUFFER END ---" {
//             if readingBuffer {
//                 taskBuffers[currentTaskID] = currentBuffer.String()
//             }
//             readingBuffer = false
//         } else if readingBuffer {
//             currentBuffer.WriteString(line + "\n")
//             if strings.HasPrefix(line, "TaskID: ") {
//                 currentTaskID, _ = strconv.Atoi(strings.TrimPrefix(line, "TaskID: "))
//             }
//         }
//     }

//     if err := scanner.Err(); err != nil {
//         return nil, err
//     }

//     lastNBuffers := make([]string, 0, n)
//     for _, buffer := range taskBuffers {
//         lastNBuffers = append(lastNBuffers, buffer)
//         if len(lastNBuffers) > n {
//             lastNBuffers = lastNBuffers[1:]
//         }
//     }
//     return lastNBuffers, nil
// }
type BufferData struct {
    Timestamp string
    Length    int
    TaskID    int
    Data      map[string]string
}
func ParseBuffers(input string) ([]BufferData, error) {
    // Split by buffer markers
    buffers := strings.Split(input, "--- BUFFER START ---")
    var results []BufferData
    
    for _, buffer := range buffers {
        if len(strings.TrimSpace(buffer)) == 0 {
            continue
        }
        
        // Remove buffer end marker
        buffer = strings.Replace(buffer, "--- BUFFER END ---", "", -1)
        
        // Create a new buffer data instance
        var bufferData BufferData
        
        // Split into lines
        lines := strings.Split(buffer, "\n")
        for _, line := range lines {
            line = strings.TrimSpace(line)
            if len(line) == 0 {
                continue
            }
            
            // Parse timestamp
            if strings.HasPrefix(line, "[") {
                timestamp := strings.Trim(line, "[]")
                bufferData.Timestamp = timestamp
            }
            
            // Parse Length
            if strings.HasPrefix(line, "Length:") {
                fmt.Sscanf(line, "Length: %d", &bufferData.Length)
            }
            
            // Parse TaskID
            if strings.HasPrefix(line, "TaskID:") {
                fmt.Sscanf(line, "TaskID: %d", &bufferData.TaskID)
            }
        }
        
        // Find JSON portion and unmarshal it
        startJSON := strings.Index(buffer, "{")
        endJSON := strings.LastIndex(buffer, "}")
        if startJSON != -1 && endJSON != -1 {
            jsonStr := buffer[startJSON : endJSON+1]
            data := make(map[string]string)
            if err := json.Unmarshal([]byte(jsonStr), &data); err != nil {
                return nil, fmt.Errorf("error unmarshaling JSON: %v", err)
            }
            bufferData.Data = data
        }
        
        results = append(results, bufferData)
    }
    
    return results, nil
}
// func GetLastNDistinctTaskBuffers(fileName string, n int) ([]map[string]int, error) {
// 	//dir := GetDistributedLogQuerierDir()
// 	filePath := filepath.Join("/Users/ingenious/Documents/DSMP1_backup/CS-425-MP/MP1/g28/Nuke/Node721/Fetched", fileName)
//     file, err := os.Open(filePath)
//     if err != nil {
//         return nil, err
//     }
//     defer file.Close()

//     taskBuffers := make(map[int]map[string]int)
//     var currentBuffer strings.Builder
//     scanner := bufio.NewScanner(file)
//     readingBuffer := false
//     var currentTaskID int

//     for scanner.Scan() {
//         line := scanner.Text()
//         if line == "--- BUFFER START ---" {
//             readingBuffer = true
//             currentBuffer.Reset()
//         } else if line == "--- BUFFER END ---" {
//             if readingBuffer {
//                 var wordCounts map[string]int
//                 err := json.Unmarshal([]byte(currentBuffer.String()), &wordCounts)
//                 if err == nil {
//                     taskBuffers[currentTaskID] = wordCounts
//                 }
//             }
//             readingBuffer = false
//         } else if readingBuffer {
//             if strings.HasPrefix(line, "{") {
//                 currentBuffer.WriteString(line)
//             } else if strings.HasPrefix(line, "TaskID: ") {
//                 currentTaskID, _ = strconv.Atoi(strings.TrimPrefix(line, "TaskID: "))
//             }
//         }
//     }

//     if err := scanner.Err(); err != nil {
//         return nil, err
//     }

//     lastNBuffers := make([]map[string]int, 0, n)
//     for _, buffer := range taskBuffers {
//         lastNBuffers = append(lastNBuffers, buffer)
//         if len(lastNBuffers) > n {
//             lastNBuffers = lastNBuffers[1:]
//         }
//     }

//     return lastNBuffers, nil
// }
//populates state in the operator directory
func PopulateStatefulOperatorFile(state map[string]int, filename string, taskID int) error {
    // Ensure the directory exists
    //dir := GetDistributedLogQuerierDir()
	//VM MARKER 
	dir := GetOperatorsDirLocal(taskID)
	//VM MARKER END
	filePath := filepath.Join(dir, filename)
    // Create or truncate the file
    file, err := os.Create(filePath)
    if err != nil {
        return fmt.Errorf("failed to create file: %v", err)
    }
    defer file.Close()
    // Write each word and its count to the file
    for word, count := range state {
        _, err := fmt.Fprintf(file, "%s:%d\n", word, count)
        if err != nil {
            return fmt.Errorf("failed to write to file: %v", err)
        }
    }

    return nil
}

func serializeLineInfoArray(tuples []LineInfo) []byte {
    var buffer bytes.Buffer
    for _, tuple := range tuples {
        buffer.WriteString(tuple.FileLineID + "|" + tuple.Content + "\n")
    }
    return buffer.Bytes()
}

func sendLineInfoArray(conn net.Conn, tuples []LineInfo, TargetNodeID string, TargetTaskID string,SelfTaskID string, SelfNodeId string) error {
    data := serializeLineInfoArray(tuples)
	//add header and footer
	data = append([]byte("INPUTBATCH: "+ TargetNodeID+" "+ TargetTaskID +" "+SelfNodeId +" "+SelfTaskID+" "+ "\n"), data...)
	data = append(data, []byte("END_OF_MSG\n")...)
    _, err := conn.Write(data)
    return err
}

func sendAckInfoArray(conn net.Conn, tuples []LineInfo, TargetNodeID string, TargetTaskID string) error {
	//ack format : ACK: targetnodeID targettaskID
    data := serializeLineInfoArray(tuples)
	//add header and footer
	data = append([]byte("ACK: "+ TargetNodeID+" "+ TargetTaskID + "\n"), data...)
	data = append(data, []byte("END_OF_MSG\n")...)
    _, err := conn.Write(data)
    return err
}

func deserializeLineInfoArray(data []byte) ([]LineInfo, error) {
	lines := strings.Split(string(data), "\n")
	tuples := make([]LineInfo, 0, len(lines))
	for _, line := range lines {
		if line == "" {
			continue
		}
		parts := strings.SplitN(line, "|", 2)
		tuples = append(tuples, LineInfo{FileLineID: parts[0], Content: parts[1]})
	}
	return tuples, nil
}

func PrintMapToConsole(inputMap map[string]string){
	fmt.Println("********Output*******")
	for key, value := range inputMap {
		fmt.Printf("\"%s\" : %v\n", key, value)
	}
}

// Helper function to split the command string, respecting quotes
func splitWithQuotes(s string) []string {
    var result []string
    var current string
    inQuotes := false
    for _, r := range s {
        switch {
        case r == '"' || r == '\'':
            inQuotes = !inQuotes
        case r == ' ' && !inQuotes:
            if current != "" {
                result = append(result, current)
                current = ""
            }
        default:
            current += string(r)
        }
    }
    if current != "" {
        result = append(result, current)
    }
    return result
}


func StoreBufferOnHydfs(bufferMap map[string] string, filename string, hydfsConn *SafeConn) {
	// Write buffer to HYDFS
	buffer_string := FormatAsBuffer(bufferMap)
	WriteBufferToBusiness(filename,buffer_string)
	//call append on file
	hydfsConn.SafeWrite([]byte("TASKAPPEND: " +  filename + " " + filename + "END_OF_MSG\n"))
	time.Sleep(1 * time.Second) // let hydfs catch up
	
}

func StoreOutputOnHydfs(outputMap map[string]string, filename string, hydfsConn *SafeConn, taskID int) {
	// Write output to HYDFS
	output_string := FormatAsOutput(outputMap, taskID)
	WriteBufferToBusiness(filename,output_string)
	//call append on file
	hydfsConn.SafeWrite([]byte("TASKAPPEND: " +  filename + " " + filename + "END_OF_MSG\n"))
	time.Sleep(1 * time.Second) // let hydfs catch up
	
}

//manip functions for source tasks
//changes per operator / functionality
//changing these will directly change the (key,value) source emits and tracks
func GetSourceKey(filename string, lineNum int) string {
	return fmt.Sprintf("%s:%d", filename, lineNum)
}
func GetSourceValue(line string) string {
	return line
}

func GetSourceLineNumberFromKey(key string) int {
	line_num := strings.Split(key, ":")[1]
	line_num_int, err := strconv.Atoi(line_num)
	if err != nil {
		fmt.Println("Key does not contain valid line number")
		return -1 
	}
	return line_num_int
}

//manip functions for stage1 tasks
//changes per operator / functionality
//changing these will directly change the (key,value) source emits and tracks
func GetStage1Key(k string, m string ) string {
	//this function generates the key for stage 1 to use which is non clashing and unique
	return fmt.Sprintf("%s:%s", k, m) //filename:linenumber:zone_id
}
func GetHashableStage1(line LineInfo) string {
	key := line.FileLineID
	//word count start
	// word_index_pair := strings.Split(key, ":")[2]
	// word := strings.Split(word_index_pair, "-")[0]
	// return word
	//word count end
	zone_id := strings.Split(key, ":")[2] //filename:linenumber:zone_id
	return zone_id
}

func GetOutputFromOperatorStage1(processed_output string) []string {
	var output_list []string
	err := json.Unmarshal([]byte(processed_output), &output_list)
	if err != nil {
		fmt.Println("Error in unmarshalling the processed output")
		fmt.Println(processed_output)
		return []string{}
	}
	if len(output_list) == 2{
		//merge the two strings into one
		merged_string := output_list[0] + "-"+output_list[1]
		return []string{merged_string}
	}else{
		return output_list
	}
	
}


func GetStage2Key(k string) string {
	return k
}

func GetInputForStage2Stateless(line LineInfo) string {
		//format of the line is filename:lineNumber:word-index
		// word_index_pair := strings.Split(line.FileLineID, ":")[2]
		// word := strings.Split(word_index_pair, "-")[0]
		// return word
		//word count end
		// zone_id := strings.Split(line.FileLineID, ":")[2]
		// return zone_id
		return line.Content
}

func GetInputForStage2Stateful(line LineInfo) string {
	cat:= strings.Split(line.FileLineID, ":")[2]
	return cat
}

func GetOutputFromOperatorStageStateful2(processed_output string) map[string]string {
	output_int_map := make(map[string]int)
	json.Unmarshal([]byte(processed_output), &output_int_map)
	//convert int values to string values
	output_string_map := make(map[string]string)
	for k, v := range output_int_map {
		output_string_map[k] = strconv.Itoa(v)
	}
	return output_string_map
}

func GetOutputFromOperatorStageStateless2(processed_output string) []string {
	var output_list []string
	err := json.Unmarshal([]byte(processed_output), &output_list)
	if err != nil {
		fmt.Println("Error in unmarshalling the processed output")
		fmt.Println(processed_output)
		return []string{}
	}
	return output_list
}

func GetStage1Content( line string, output string) string {
	return line
}

//manip functions end

func ClearOperatorStatelocal(filename string, taskID int){
	dir := GetOperatorsDirLocal(taskID)
	filePath := filepath.Join(dir, filename)
	err := os.Remove(filePath)
	if err != nil {
		fmt.Println("Error deleting operator state file:", err)
	}
	//recreate a new file 	
	file, err := os.Create(filePath)
	if err != nil {
		fmt.Println("Error creating operator state file:", err)
	}
	defer file.Close()
}

func ClearOperatorState(filename string){
	for i := 0; i < 10; i++ {
		ClearOperatorStatelocal(filename, i)
	}
}


func ReadFilePartition(filename string, start, end int) ([]LineInfo, error) {
	dir := GetDistributedLogQuerierDir()
	filePath := filepath.Join(dir, "Fetched", filename)
    file, err := os.Open(filePath)
    if err != nil {
        return nil, err
    }
    defer file.Close()
	fileInfo, err := file.Stat()
	if err != nil {
		return nil, err
	}
	if fileInfo.Size() == 0 {
		return nil, fmt.Errorf("file is empty: %s", filePath)
	}
    scanner := bufio.NewScanner(file)
	//650 kb buffer
	const maxCapacity = 800 * 1024
    buf := make([]byte, maxCapacity)
    scanner.Buffer(buf, maxCapacity)
	
    lines := []LineInfo{}
    lineNum := 0

    for scanner.Scan() {
        lineNum++
        if lineNum >= start && lineNum <= end {
            fileLineID := GetSourceKey(filename, lineNum)
			lineContent := GetSourceValue(scanner.Text())
            lines = append(lines, LineInfo{
                FileLineID: fileLineID,
                Content:    lineContent,
            })
        }
        if lineNum > end {
            break
        }
    }

    if err := scanner.Err(); err != nil {
        return nil, err
    }

    return lines, nil
}

func NewTask(taskID int, taskNodeID int , taskType string, taskStatus string, taskStage string, taskInputType string, taskOutputType string, taskOutputNodes []string, taskOutputFile string, taskInputFile string, taskPartitionRange string, taskInputNode string, taskState string, taskOperatorName string, taskReceiveAck bool, doReplay bool, TaskOutputIDs []int, num_tasks int, taskOperatorInput []string, InputTaskID int) *Task {
	return &Task{
		TaskID: taskID,
		TaskAssignedNode: taskNodeID,
		TaskName: "task" + strconv.Itoa(taskID),
		TaskLogFile: "task" + strconv.Itoa(taskID) + ".txt",
		TaskType: taskType,
		TaskStatus: taskStatus,
		TaskStage: taskStage,
		TaskInputType: taskInputType,
		TaskOutputType: taskOutputType,
		TaskOutputNodes: taskOutputNodes,
		TaskOutputFile: taskOutputFile,
		TaskInputFile: taskInputFile,
		TaskPartitionRange: taskPartitionRange,
		TaskInputNode: taskInputNode,
		TaskState: taskState,
		TaskOperatorName: taskOperatorName,
		TaskReceiveAck: taskReceiveAck,
		DoReplay: doReplay,
		OutputTaskIDs: TaskOutputIDs, 
		NumTasks: num_tasks,
		TaskOperatorInput: taskOperatorInput,
		InputTaskID: InputTaskID,
	}
}

func GetFileFromHydfs(hydfsConn *SafeConn,filename string, waitLatencyMS int) bool {
	message := "TASKGET: " + filename + " " + filename
	hydfsConn.SafeWrite([]byte (message + "END_OF_MSG\n"))
	return WaitOnFile(filename, waitLatencyMS)
}

//used for removing failed node ids from the list and insert new node ids
func mergeTaskList(old_node_list []string, new_node_list []int, failed_node_list []int) []string {
	fmt.Println("Old node list : ", old_node_list)
	fmt.Println("New node list : ", new_node_list)
	fmt.Println("Failed node list : ", failed_node_list)
	old_node_list_int := make([]int, 0)
	for _, node_id := range old_node_list {
		node_id_int, err := strconv.Atoi(node_id)
		if err != nil {
			fmt.Println("Error converting node id to int")
			continue
		}
		old_node_list_int = append(old_node_list_int, node_id_int)
	}
	//remove failed node ids
	removed_failed_node_ids := make([]int, 0)
	for _, node_id := range old_node_list_int {
		if !slices.Contains(failed_node_list, node_id){
			removed_failed_node_ids = append(removed_failed_node_ids, node_id)
		}
	}
	//add new node ids
	removed_failed_node_ids = append(removed_failed_node_ids, new_node_list...)
	//convert back to string
	removed_failed_node_ids_str := make([]string, 0)
	for _, node_id := range removed_failed_node_ids {
		removed_failed_node_ids_str = append(removed_failed_node_ids_str, strconv.Itoa(node_id))
	}
	fmt.Println("Merged node list : ", removed_failed_node_ids_str)
	return removed_failed_node_ids_str
}

func sendResumeMessageToAllStage1Nodes(streamTaskTable *sync.Map, streamConnTable *sync.Map, output_node_ids []string){
	fmt.Println("Sending resume message to all stage 1 nodes")
		streamTaskTable.Range(func(key, value interface{}) bool {
			tasks := value.([]*Task)
			for _, taskitr := range tasks {
				if taskitr.TaskType == "operator" && taskitr.TaskStage == "first" {
					//send the resume message to the input node
					taskNodeID := taskitr.TaskAssignedNode
					taskNodeConn, ok := streamConnTable.Load(taskNodeID)
					if !ok {
						fmt.Println("Input node connection not found in send resume ", taskNodeID)
						return true
					}
					//update the output nodes of the task
					taskitr.TaskOutputNodes = output_node_ids
					//RESUME send format RESUME: targetNodeID targettaskID+new_output_node_id/s (PS. the targetNodeID is filtered out once receipt)
					resume_msg := "RESUME: " + strconv.Itoa(taskitr.TaskAssignedNode) + " " + strconv.Itoa(taskitr.TaskID) + "+" + strings.Join(taskitr.TaskOutputNodes, " ")
					time.Sleep(20 * time.Millisecond)
					taskNodeConn.(net.Conn).Write([]byte(resume_msg + "END_OF_MSG\n"))
				}
			}
			return true
		})
}

func SortNodesByTaskIDs(nodes []string, taskIDs []int) ([]string, []int) {
    if len(nodes) != len(taskIDs) {
        fmt.Println("Error: Mismatch in length of nodes and taskIDs")
        return nodes, taskIDs
    }

    type NodeTaskPair struct {
        node   string
        taskID int
    }

    pairs := make([]NodeTaskPair, len(nodes))
    for i := range nodes {
        pairs[i] = NodeTaskPair{node: nodes[i], taskID: taskIDs[i]}
    }

    sort.Slice(pairs, func(i, j int) bool {
        return pairs[i].taskID < pairs[j].taskID
    })

    sortedNodes := make([]string, len(nodes))
    sortedTaskIDs := make([]int, len(taskIDs))
    for i, pair := range pairs {
        sortedNodes[i] = pair.node
        sortedTaskIDs[i] = pair.taskID
    }

    return sortedNodes, sortedTaskIDs
}

//serializes the output node ids according to the taskids and returns the ordered list
//useful to maintain nodeid->taskid mapping for resuming tasks
type OutputNodeTaskPair struct{
	nodeID int
	taskID int
}
func serializeOutputNodes(outputNodes []string,streamTaskTable *sync.Map) [] string{ 
	fmt.Println("Output nodes not serial : " , outputNodes)
	nodeTask_stage2_pairs := make([]OutputNodeTaskPair, 0)
	streamTaskTable.Range(func(key, value interface{}) bool {
		tasks := value.([]*Task)
		for _, taskitr := range tasks {
			if taskitr.TaskType == "operator" && taskitr.TaskStage == "second" {
				nodeTask_stage2_pairs = append(nodeTask_stage2_pairs, OutputNodeTaskPair{nodeID: taskitr.TaskAssignedNode, taskID: taskitr.TaskID})
			}
		}
		return true
	})
	//sort the pairs by taskID
	sort.Slice(nodeTask_stage2_pairs, func(i, j int) bool {
		return nodeTask_stage2_pairs[i].taskID < nodeTask_stage2_pairs[j].taskID
	})
	//serialize the output nodes
	output_node_ids := make([]string, 0)
	for _, pair := range nodeTask_stage2_pairs {
		output_node_ids = append(output_node_ids, strconv.Itoa(pair.nodeID))
	}
	//perform sanity check to ensure the output nodes match 
	for _, nodeID := range outputNodes {
		if !slices.Contains(output_node_ids, nodeID){
			fmt.Println("Sanity check failed Output node not found in the serialized list")
			fmt.Println("Output nodes sent : " , outputNodes)
			fmt.Println("Output nodes serialized : ", output_node_ids)

		}
	}
	fmt.Println("Output nodes serial : " , output_node_ids)
	return output_node_ids
}
func printStreamTaskTable(streamTaskTable *sync.Map){
	fmt.Println("Stream Task Table : ", getSyncMapLength(streamTaskTable))
	streamTaskTable.Range(func(key, value interface{}) bool {
		fmt.Print(key, " : ")
		tasks := value.([]*Task)
		for _, task := range tasks {
			fmt.Print(task.TaskID," ")
		}
		fmt.Println()
		return true
	})
}

func GetNodeIDForTask(taskID int, streamTaskTable *sync.Map) int {
	nodeID := -1
	streamTaskTable.Range(func(key, value interface{}) bool {
		tasks := value.([]*Task)
		for _, taskitr := range tasks {
			if taskitr.TaskID == taskID {
				nodeID = taskitr.TaskAssignedNode
				return false
			}
		}
		return true
	})
	return nodeID

}

func GetSampleOutputNodes(streamTaskTable* sync.Map) []string {
	var outputNodes []string
	streamTaskTable.Range(func(key, value interface{}) bool {
		tasks := value.([]*Task)
		for _, task := range tasks {
			if task.TaskType == "operator" && task.TaskStage == "first" {
				outputNodes = task.TaskOutputNodes
				return false // stop iteration
			}
			
		}
		return true // continue iteration
	})
	return outputNodes
}

func GetNodeWithLeastTasks(streamTaskTable *sync.Map) int {
	minTasks := 100000 
	minNode := -1
	streamTaskTable.Range(func(k, v interface{}) bool {
		tasks := v.([]*Task)
		if len(tasks) < minTasks {
			minTasks = len(tasks)
			minNode = k.(int)
		}
		return true
	})
	return minNode
}

//blocks till the file is available in the fetched folder
func WaitOnFile(fileName string, milliseconds int) bool {
    dir := GetDistributedLogQuerierDir()
    fetched_dir := filepath.Join(dir, "Fetched")
    filePath := filepath.Join(fetched_dir, fileName)
    
    timeout := time.After(5 * time.Second)
    ticker := time.NewTicker(time.Duration(milliseconds) * time.Millisecond)
    defer ticker.Stop() // Ensure the ticker is stopped when the function 

    for {
        select {
        case <-timeout:
            fmt.Printf("Timeout waiting for file: %s\n", fileName)
            return false
        case <-ticker.C:
            if _, err := os.Stat(filePath); err == nil {
                return true
            }
        }
    }
}

func FormatAsOutput(input map[string]string, taskID int) string{
	timestamp := time.Now().Format(timeFormat)
	// Convert map to JSON string
	jsonData, err := json.MarshalIndent(input, "", "  ")
	if err != nil {
		// In case of marshaling error, return error as part of formatted string
		jsonData = []byte(fmt.Sprintf("Error marshaling data: %v", err))
	}
	
	formatted := fmt.Sprintf("%s\n[%s] Length: %d\nTaskID: %d\n%s\n%s",
		bufferStart,
		timestamp,
		len(jsonData),
		taskID,
		string(jsonData),
		bufferEnd)
	return formatted
}

// FormatAsBuffer formats a map as a buffer string
func FormatAsBuffer(input map[string]string) string {
	timestamp := time.Now().Format(timeFormat)
	
	// Convert map to JSON string
	jsonData, err := json.MarshalIndent(input, "", "  ")
	if err != nil {
		// In case of marshaling error, return error as part of formatted string
		jsonData = []byte(fmt.Sprintf("Error marshaling data: %v", err))
	}
	
	formatted := fmt.Sprintf("%s\n[%s] Length: %d\n%s\n%s",
		bufferStart,
		timestamp,
		len(jsonData),
		string(jsonData),
		bufferEnd)
	return formatted
}


//solves the problem of text -> file for appends
//used business instead of temp 
//it will overwrite so no need to worry about existing files
func WriteBufferToBusiness(filename string, buffer string){
	dir := GetDistributedLogQuerierDir()
	temp_dir := filepath.Join(dir, "business")
	//check if the temp directory exists
	if _, err := os.Stat(temp_dir); os.IsNotExist(err) {
		os.Mkdir(temp_dir, 0755)
	}
	//write the buffer to the temp file
	filePath := filepath.Join(temp_dir, filename)
	err := os.WriteFile(filePath, []byte(buffer), 0644)
	if err != nil {
		fmt.Println("Error writing buffer to temp business for append:", err)
	}
}

//used for local TESTING only!
func WriteBufferToFileTestOnly(buffer string, filename string) {
	dir := GetDistributedLogQuerierDir()
	bufferFile := filepath.Join(dir, "fetched", filename)
	
	// Open file for writing
	file, err := os.OpenFile(bufferFile, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		fmt.Printf("Error opening buffer file: %v\n", err)
		return
	}
	defer file.Close()
	
	// Write buffer to file
	_, err = file.WriteString(buffer + "\n")
	if err != nil {
		fmt.Printf("Error writing buffer to file: %v\n", err)
	}
}
func ReadLastBufferForTask(filename string, taskID int) (map[string]string, error) {
    if !strings.Contains(filename, ".txt") {
        filename += ".txt"
    }
    
    dir := GetDistributedLogQuerierDir()
    fetched_dir := filepath.Join(dir, "Fetched")
    bufferFile := filepath.Join(fetched_dir, filename)
    
    // Check if file exists
    file, err := os.Open(bufferFile)
    if err != nil {
        if os.IsNotExist(err) {
            return nil, nil
        }
        return nil, fmt.Errorf("failed to open buffer file: %v", err)
    }
    defer file.Close()
    
    // Get file size
    fileInfo, err := file.Stat()
    if err != nil {
        return nil, fmt.Errorf("failed to get file info: %v", err)
    }
    fileSize := fileInfo.Size()
    
    // Start from the end of the file
    var buffer strings.Builder
    var lastBuffer string
    position := fileSize
    foundStart := false
    foundEnd := false
    foundTaskID := false
    
    for position > 0 && !foundStart {
        // Determine chunk size
        chunkSize := int64(readChunkSize)
        if position < chunkSize {
            chunkSize = position
        }
        position -= chunkSize
        
        // Read chunk
        chunk := make([]byte, chunkSize)
        _, err := file.ReadAt(chunk, position)
        if err != nil && err != io.EOF {
            return nil, fmt.Errorf("failed to read file chunk: %v", err)
        }
        
        // Prepend chunk to our buffer
        buffer.Write(chunk)
        content := buffer.String()
        
        // Look for buffer markers
        if !foundEnd {
            endIndex := strings.LastIndex(content, bufferEnd)
            if endIndex != -1 {
                content = content[:endIndex]
                foundEnd = true
            }
        }
        
        if foundEnd {
            startIndex := strings.LastIndex(content, bufferStart)
            if startIndex != -1 {
                lastBuffer = content[startIndex+len(bufferStart):]
                foundStart = true
                
                // Check if this buffer is for the correct taskID
                if strings.Contains(lastBuffer, fmt.Sprintf("TaskID: %d", taskID)) {
                    foundTaskID = true
                    break
                } else {
                    // If not, reset and continue searching
                    foundStart = false
                    foundEnd = false
                    buffer.Reset()
                }
            }
        }
    }
    
    if !foundStart || !foundEnd || !foundTaskID {
        return nil, nil
    }
    
    // Clean up the buffer and extract JSON content
    cleanBuffer := strings.TrimSpace(lastBuffer)
    
    // Find the JSON content after the timestamp and TaskID lines
    lines := strings.Split(cleanBuffer, "\n")
    if len(lines) < 4 { // We need at least timestamp line, TaskID line, and some JSON content
        return nil, fmt.Errorf("EMPTY BUFFER")
    }
    
    // Find the start of JSON content (looking for '{')
    var jsonContent string
    for i, line := range lines {
        if strings.Contains(line, "{") {
            jsonContent = strings.Join(lines[i:], "\n")
            break
        }
    }
    
    if jsonContent == "" {
        return nil, fmt.Errorf("no valid JSON content found in buffer")
    }
    
    // Parse JSON into map[string]string
    result := make(map[string]string)
    if err := json.Unmarshal([]byte(jsonContent), &result); err != nil {
        return nil, fmt.Errorf("failed to parse buffer content: %v, content was: %s", err, jsonContent)
    }
    
    return result, nil
}

// ReadLastBuffer reads and returns the most recent buffer from the file
func ReadLastBuffer(filename string) (map[string]string, error) {
	if !strings.Contains(filename, ".txt") {
		filename += ".txt"
	}
	
	dir := GetDistributedLogQuerierDir()
	fetched_dir := filepath.Join(dir, "fetched")
	bufferFile := filepath.Join(fetched_dir, filename)
	
	// Check if file exists
	file, err := os.Open(bufferFile)
	if err != nil {
		if os.IsNotExist(err) {
			return nil, nil
		}
		return nil, fmt.Errorf("failed to open buffer file: %v", err)
	}
	defer file.Close()
	
	// Get file size
	fileInfo, err := file.Stat()
	if err != nil {
		return nil, fmt.Errorf("failed to get file info: %v", err)
	}
	fileSize := fileInfo.Size()
	
	// Start from the end of the file
	var buffer strings.Builder
	var lastBuffer string
	position := fileSize
	foundStart := false
	foundEnd := false
	
	for position > 0 && !foundStart {
		// Determine chunk size
		chunkSize := int64(readChunkSize)
		if position < chunkSize {
			chunkSize = position
		}
		position -= chunkSize
		
		// Read chunk
		chunk := make([]byte, chunkSize)
		_, err := file.ReadAt(chunk, position)
		if err != nil && err != io.EOF {
			return nil, fmt.Errorf("failed to read file chunk: %v", err)
		}
		
		// Prepend chunk to our buffer
		buffer.Write(chunk)
		content := buffer.String()
		
		// Look for buffer markers
		if !foundEnd {
			endIndex := strings.LastIndex(content, bufferEnd)
			if endIndex != -1 {
				content = content[:endIndex]
				foundEnd = true
			}
		}
		
		if foundEnd {
			startIndex := strings.LastIndex(content, bufferStart)
			if startIndex != -1 {
				lastBuffer = content[startIndex+len(bufferStart):]
				foundStart = true
				break
			}
		}
	}
	
	if !foundStart || !foundEnd {
		return nil, nil
	}
	
	// Clean up the buffer and extract JSON content
	cleanBuffer := strings.TrimSpace(lastBuffer)
	
	// Find the JSON content after the timestamp line
	lines := strings.Split(cleanBuffer, "\n")
	if len(lines) < 3 { // We need at least timestamp line and some JSON content
		return nil, fmt.Errorf("EMPTY BUFFER")
	}
	
	// Find the start of JSON content (looking for '{')
	var jsonContent string
	for i, line := range lines {
		if strings.Contains(line, "{") {
			jsonContent = strings.Join(lines[i:], "\n")
			break
		}
	}
	
	if jsonContent == "" {
		return nil, fmt.Errorf("no valid JSON content found in buffer")
	}
	
	// Parse JSON into map[string]string
	result := make(map[string]string)
	if err := json.Unmarshal([]byte(jsonContent), &result); err != nil {
		return nil, fmt.Errorf("failed to parse buffer content: %v, content was: %s", err, jsonContent)
	}
	
	return result, nil
}
type SafeConn struct {
    conn net.Conn
    mu   sync.Mutex
}

type StringIntPair struct {
	Key   string
	Value int
}

type LamportClock struct {
    timestamp int64
}

func (sc *SafeConn) SafeWrite(data []byte) (int, error) {
    sc.mu.Lock()
    defer sc.mu.Unlock()
    return sc.conn.Write(data)
}

// Increment safely increments the Lamport timestamp
func (lc *LamportClock) Increment() int64 {
    return atomic.AddInt64(&lc.timestamp, 1)
}

//does not handle closing task channel and removing it from the taskChannelTable
func EndTask(leaderConn net.Conn, streamTaskTable *sync.Map, task *Task) {
	self_id := getSelf_id()
	_, err := leaderConn.Write([]byte("COMPLETEDTASK: " +strconv.Itoa(self_id) +" "+strconv.Itoa(task.TaskID) + "END_OF_MSG\n"))
	if err != nil {
		fmt.Println("Error sending task completion message to leader")
	}
	//remove the task from the streamTaskTable
	pre_tasks, ok  := streamTaskTable.Load(self_id)
	if ok{
		tasks := pre_tasks.([]*Task)
		new_tasks := make([]*Task, 0)
		for _, t := range tasks {
			if t.TaskID != task.TaskID {
				new_tasks = append(new_tasks, t)
			}
		}
		streamTaskTable.Store(self_id, new_tasks)
		fmt.Println("Task removed from the streamTaskTable successfully")
	}else{
		fmt.Println("Task to remove not found in the streamTaskTable")
	}
}

func RelayOutputToLeader(leaderConn net.Conn, output_map map[string]string, task *Task) {
    outputJSON, err := json.Marshal(output_map)
    if err != nil {
        fmt.Println("Error marshaling output map:", err)
        return
    }
    message := fmt.Sprintf("TASKINFO: %d %d\n%sEND_OF_MSG\n", self_id, task.TaskID, outputJSON)
    _, err = leaderConn.Write([]byte(message))
    if err != nil {
        fmt.Println("Error sending task info to leader:", err)
    }
}

// returns the paritions as a list(int) of lists like [[1,300],[301,600],[601,900]] for 900 lines and 3 tasks
func GetFairPartitions(lines int, num_tasks int) [][]int {
	if num_tasks == 0 {
		return nil
	}
	if num_tasks > lines {
		num_tasks = lines
	}
    base_partition_size := lines / num_tasks
    remainder := lines % num_tasks
    partitions := make([][]int, num_tasks)

    start := 1
    for i := 0; i < num_tasks; i++ {
        end := start + base_partition_size - 1
        if i == num_tasks-1 {
            end += remainder // Add remainder to the last partition
        }
        partitions[i] = []int{start, end}
        start = end + 1
    }

    return partitions
}


func CountLines(fileName string) (int, error) {
	//read from fetched dir 
	dir := GetDistributedLogQuerierDir()
	filePath := filepath.Join(dir, "Fetched", fileName)
    file, err := os.Open(filePath)
    if err != nil {
        return 0, err
    }
    defer file.Close()

    count := 0
    buf := make([]byte, 32*1024)
    lineSep := []byte{'\n'}

    for {
        c, err := file.Read(buf)
        count += bytes.Count(buf[:c], lineSep)

        switch {
        case err == io.EOF:
            return count, nil
        case err != nil:
            return count, err
        }
    }
}
func RunOperatorlocal(operator_name string, parameter string, input string, taskID int) string {
	operator_dir := GetOperatorsDirLocal(taskID)
	operator_path := filepath.Join(operator_dir, operator_name)
	//fmt.Println("Operator path: ", operator_path)
	// Create command
	cmd := exec.Command(operator_path)
	
	// Create pipes for stdin, stdout, and stderr
	stdin, err := cmd.StdinPipe()
	if err != nil {
		return fmt.Sprintf("Error creating stdin pipe: %v", err)
	}

	// Capture both stdout and stderr
	var stdout, stderr bytes.Buffer
	cmd.Stdout = &stdout
	cmd.Stderr = &stderr

	// Start the command
	if err := cmd.Start(); err != nil {
		return fmt.Sprintf("Error starting operator: %v", err)
	}

	// Write input to stdin and close it
	if parameter == "None"{
		_, err = fmt.Fprintln(stdin, input)  // Use Fprintln to add newline
		if err != nil {
			fmt.Println(operator_path, parameter, input, taskID)
			return fmt.Sprintf("Error writing to stdin1: %v", err)
		}
	}else{
		// Write parameter and input to stdin, separated by newline
		//parameter passed first
		_, err = fmt.Fprintf(stdin, "%s\n%s\n", parameter, input)
		if err != nil {
			fmt.Println(operator_path, parameter, input, taskID)
			return fmt.Sprintf("Error writing to stdin2: %v", err)
		}
	}
	stdin.Close()

	// Wait for command to finish
	err = cmd.Wait()
	if err != nil {
		// If there's stderr output, return it
		if stderr.Len() > 0 {
			return fmt.Sprintf("Operator error: %s", stderr.String())
		}
		return fmt.Sprintf("Error running operator: %v", err)
	}

	result := strings.TrimSpace(stdout.String())
	return result
}


// func RunOperator(operator_name string, parameter string, input string) string {
// 	 operator_dir := GetOperatorsDir()
// 	 operator_path := filepath.Join(operator_dir, operator_name)
// 	//operator_path :="/Users/ingenious/Documents/DSMP1_backup/CS-425-MP/MP1/g28/distributed_log_querier/count_op_mac"
// 	// Create command
// 	cmd := exec.Command(operator_path)
	
// 	// Create pipes for stdin, stdout, and stderr
// 	stdin, err := cmd.StdinPipe()
// 	if err != nil {
// 		return fmt.Sprintf("Error creating stdin pipe: %v", err)
// 	}

// 	// Capture both stdout and stderr
// 	var stdout, stderr bytes.Buffer
// 	cmd.Stdout = &stdout
// 	cmd.Stderr = &stderr

// 	// Start the command
// 	if err := cmd.Start(); err != nil {
// 		return fmt.Sprintf("Error starting operator: %v", err)
// 	}

// 	// Write input to stdin and close it
// 	if parameter == "None"{
// 		_, err = fmt.Fprintln(stdin, input)  // Use Fprintln to add newline
// 		if err != nil {
// 			return fmt.Sprintf("Error writing to stdin: %v", err)
// 		}
// 	}else{
// 		// Write parameter and input to stdin, separated by newline
// 		//parameter passed first
// 		_, err = fmt.Fprintf(stdin, "%s\n%s\n", parameter, input)
// 		if err != nil {
// 			return fmt.Sprintf("Error writing to stdin: %v", err)
// 		}
// 	}
// 	stdin.Close()

// 	// Wait for command to finish
// 	err = cmd.Wait()
// 	if err != nil {
// 		// If there's stderr output, return it
// 		if stderr.Len() > 0 {
// 			return fmt.Sprintf("Operator error: %s", stderr.String())
// 		}
// 		return fmt.Sprintf("Error running operator: %v", err)
// 	}

// 	result := strings.TrimSpace(stdout.String())
// 	return result
// }

func RingRepair(lc *LamportClock,connTable * sync.Map, KeyTable *sync.Map, fileNameMap *sync.Map){
	//steps
	//1. check if you hold any file that should be on the new node, if yes send it and the appends on it and remove it from your fileBay and append bay
	//2. check if you hold the second replica for some file that you no longer need to maintain, if yes simply remove it
	//do a directory walk on fileBay
	dir := GetDistributedLogQuerierDir()
	filebay_dir := filepath.Join(dir, "FileBay")
	//walk over the file bay
	filebay_files, err := os.ReadDir(filebay_dir)
	if err != nil {
		fmt.Println("no files in filebay right now", err)
	}
	self_id := getSelf_id()
	for _, file := range filebay_files {
		base := strings.Split(file.Name(), ".")[0]
		fileID_str := strings.Split(base, "_")[1]
		fileID , err := strconv.Atoi(fileID_str)
		if err != nil {
			fmt.Println("Error converting fileID to int in ring repair ", fileID_str , err)
		}
		cood_id := GetHyDFSCoordinatorID(KeyTable, fileID)
		if cood_id != self_id{
			fmt.Println("Moving file to new node ", cood_id)
			conn, ok  := connTable.Load(cood_id)
			if !ok{
				fmt.Println("No connection found to the new node")
				continue
			}
			//send the MAPSFILE: message to the new node
			fmt.Println("Sending file map to the new node", cood_id)
			sendFileMap(lc, conn.(net.Conn), fileNameMap)
			forwardOriginal(lc, conn.(net.Conn), fileID_str, fileNameMap)
			//send remove replica message to the third successor who no longer needs to maintain the replica
			_, second_successor_id:= GetHYDFSSuccessorIDs(self_id, KeyTable)
			//third_successor_id, _ := GetHYDFSSuccessorIDs(second_successor_id, KeyTable)
			fmt.Println("Removing replica from ", second_successor_id, " for file ", fileID_str)
			fmt.Println(self_id, " ", second_successor_id, " ")
			//
			conn, ok  = connTable.Load(second_successor_id)
			if !ok{
				fmt.Println("No connection found to the third successor")
				continue
			}
			fmt.Println("Removing file ", file.Name())
			removeFile(filepath.Join(filebay_dir, file.Name()))
			sendHyDFSMessage(lc, conn.(net.Conn), "REMOVEREPLICA: " + fileID_str)

		}else{
			//send best effort replica removal message to the third successor
			_, second_successor_id:= GetHYDFSSuccessorIDs(self_id, KeyTable)
			third_successor_id, _ := GetHYDFSSuccessorIDs(second_successor_id, KeyTable)
			fmt.Println("Removing replica from if it exists ", third_successor_id, " for file ", fileID_str)
			fmt.Println(self_id, " ", third_successor_id, " ")
			//
			conn, ok  := connTable.Load(third_successor_id)
			if !ok{
				fmt.Println("No connection found to the third successor")
				continue
			}
			sendHyDFSMessage(lc, conn.(net.Conn), "REMOVEREPLICA: " + fileID_str)
			
		}

	}
	
}

func sendFileMap(lc *LamportClock, conn net.Conn, fileNameMap *sync.Map) {
	//iterate over sync map 
	fileNameMap.Range(func(k, v interface{}) bool {
		fileID := k.(string)
		hydfsFileName := v.(string)
		sendHyDFSMessage(lc, conn, "MAPSFILE: " + fileID + " " + hydfsFileName)
		return true
	})

}

func GetHYDFSSuccessorIDs(self_id int, keytable *sync.Map) (int, int){
	//get the wrapped 2 successors of the current node
	//get the keys from the table and then sort them
	keys := make([]int, 0)
	keytable.Range(func(k, v interface{}) bool {
		keys = append(keys, k.(int))
		return true
	})
	if len(keys) < 3{
		fmt.Println("Not enough keys in the keyTable")
		return -1, -1
	}
	
	//sort the keys
	sort.Ints(keys)
	var index int 
	//find the position of the key in the sorted list
	for i, key := range keys{
		if key == self_id{
			index = i
			break
		}
	}
	if index == len(keys)-1{
		return keys[0], keys[1]
	}
	if index == len(keys)-2{
		return keys[index+1], keys[0]
	}
	return keys[index+1], keys[index+2]
}

// GetTimestamp safely retrieves the current timestamp
func (lc *LamportClock) GetTimestamp() int64 {
    return atomic.LoadInt64(&lc.timestamp)
}

func GetTasks(arr []int, cond string) (p int ,q int){
	x := string(cond[len(cond)-1])
	if x == "1" {return arr[len(arr)-2], arr[len(arr)-1]}
	if x == "2" {return arr[0], arr[1]}else{return arr[len(arr)-2], arr[len(arr)-1]}
}


//file to show return the values stored locally in all the bays (used for store command)
func ShowLocalFiles(fileNameMap *sync.Map){
	dir := GetDistributedLogQuerierDir()
	//show the files in the file bay
	fmt.Println("Showing storage at " + strconv.Itoa(getSelf_id()) )
	fmt.Println("Original Files")
	files, err := os.ReadDir(filepath.Join(dir, "FileBay"))
	if err != nil {
		fmt.Println("Error reading file bay directory:", err)
	}
	for _, file := range files {
		base := strings.Split(file.Name(), ".")[0]
		hydfsName, ok:= fileNameMap.Load(strings.Split(base, "_")[1])
		if ok{
			fmt.Println(file.Name() + " - " + hydfsName.(string))
		}else{
			fmt.Println(file.Name())
		}
	}
	//show the files in the replica bay
	fmt.Println("Replica Files")
	files, err =
		os.ReadDir(filepath.Join(dir, "ReplicaBay"))
	if err != nil {
		fmt.Println("Error reading replica bay directory:", err)
	}
	for _, file := range files {
		base := strings.Split(file.Name(), ".")[0]
		hydfsName, ok:= fileNameMap.Load(strings.Split(base, "_")[1])
		if ok{
			fmt.Println(file.Name() + " - " + hydfsName.(string))
		}else{
			fmt.Println(file.Name())
		}
	}
	//show the files in the append bay
	fmt.Println("Logical Append Folders")
	files, err = os.ReadDir(filepath.Join(dir, "appendBay"))
	if err != nil {
		fmt.Println("Error reading append bay directory:", err)
	}
	for _, file := range files {
		fmt.Println("Logical append from " + file.Name())
	}
	fmt.Println("**********")
}

//returns the message containing the local support for the file, to be used in ls hydfsfilename command
func GetLocalSupportForFile(fileID string) string{
	support:= ""
	self_id := getSelf_id()
	dir := GetDistributedLogQuerierDir()
	//check if the file exists in the file bay
	selfAddress := GetOutboundIP().String()
	if checkFileExists("FileBay", "original_" + fileID){
			support = support + "Coordinator " + strconv.Itoa(self_id) + "@"+ selfAddress+ " " +"stores the file original_"+ fileID + ".txt\n"
	}
	//check if the file exists in the replica bay
	if checkFileExists("ReplicaBay", "replica_" + fileID){
		support = support + "Replica " + strconv.Itoa(self_id) + "@"+ selfAddress+ " " +"stores the file replica_"+ fileID + ".txt\n"
	}
	//check if the file exists in the append bay
	appendDir := filepath.Join(dir, "appendBay")
	nodeDirs, err := os.ReadDir(appendDir)
	if err != nil {
		fmt.Println("Error reading append directory:", err)
		return ""
	}
	//ensure support has some content
	if len(nodeDirs) == 0{
		return support
	}
	nodeDirNames := make([]string, 0)
	flag := false
	for _, nodeDir := range nodeDirs {
		//directly send nodedir names 
		//if node dir is empty then skip 
		isEmpty, err := isDirEmpty(filepath.Join(appendDir, nodeDir.Name()))
		if err != nil {
			fmt.Println("Error checking if directory is empty:", err)
			return support
		}
		if !isEmpty{
			nodeDirNames = append(nodeDirNames, nodeDir.Name())
			flag = true
		}
	}
	if flag{
		support = support + "Append Files " + strconv.Itoa(self_id) + "@"+ selfAddress + " stores logical appends from " + strings.Join(nodeDirNames, ",") + "\n"
	}
		return support
}

func isDirEmpty(dir string) (bool, error) {
    f, err := os.Open(dir)
    if err != nil {
        return false, err
    }
    defer f.Close()

    // Read only one entry from the directory
    _, err = f.Readdir(1)
    if err == io.EOF {
        // No entries found, so the directory is empty
        return true, nil
    }
    if err != nil {
        // Return any other error encountered
        return false, err
    }
    // Entry exists, so the directory is not empty
    return false, nil
}

func cleanlocalFileSystem(){
	dir := GetDistributedLogQuerierDir()
	//remove the directory fetched
	deleteDirectoriesExceptBusiness(dir)
	GetDistributedLogQuerierDir() // to remake the directory
}

func RemoveReplicaLocal(fileID string){
	filename:= "replica_" + fileID + ".txt"
	if checkFileExists("ReplicaBay", filename){
		removeFile(filepath.Join(GetDistributedLogQuerierDir(), "ReplicaBay", filename))
		
		//remove the logical appends for this file
		appendDir := filepath.Join(GetDistributedLogQuerierDir(), "appendBay")
		nodeDirs, err := os.ReadDir(appendDir)
		if err != nil {
			fmt.Println("Error reading append directory:", err)
			return
		}
		for _, nodeDir := range nodeDirs {
			nodeDirPath := filepath.Join(appendDir, nodeDir.Name())
			files, err := os.ReadDir(nodeDirPath)
			if err != nil {
				fmt.Println("Error reading append directory:", err)
				continue
			}
			for _, file := range files {
				fileName := file.Name()
				fileID_cur := strings.Split(fileName, "_")[2]
				if fileID_cur == fileID{
					removeFile(filepath.Join(nodeDirPath, fileName))
				}
			}
		}
		fmt.Println("Replica removed for file " + fileID)
	}
}

//handles the forwarding of both, append files and replicas
func forwardReplica(lc *LamportClock, conn net.Conn, fileID string){
	file_name:= "original_" + fileID + ".txt"
	send_file_name:= "replica_" + fileID + ".txt"
	sendHyDFSFile(lc, conn, "original", file_name, send_file_name)
	//send the logical append files 
	//check if there are any logical appends for this file 
	dir := GetDistributedLogQuerierDir()
	appendDir := filepath.Join(dir, "appendBay")
	nodeDirs, err := os.ReadDir(appendDir)
	if err != nil {
		fmt.Println("Error reading append directory:", err)
		return
	}
	//read the nodeDir folders
	for _, nodeDirF := range nodeDirs {
		nodeDir := filepath.Join(appendDir, nodeDirF.Name())
		nodeID := nodeDirF.Name()
		if _, err := os.Stat(nodeDir); os.IsNotExist(err) {
			fmt.Println("No logical appends found for file - NO OP " + fileID)
			return
		}
		files, err := os.ReadDir(nodeDir)
		if err != nil {
			fmt.Println("Error reading append directory:", err)
			return
		}
		//send the logical append files
		for _, file := range files {
				node_file_path:= filepath.Join(nodeID, file.Name())
				fmt.Println("Node file path for append" + node_file_path)
				fmt.Println("Sending logical append file " + file.Name() + " to " + conn.RemoteAddr().String())
				sendHyDFSFile(lc, conn, "append", node_file_path, file.Name())
		}
	}
}

//handles the forwarding of both, append files and original file
func forwardOriginal(lc *LamportClock, conn net.Conn, fileID string, fileNameMap *sync.Map){
	file_name:= "original_" + fileID + ".txt"
	hydfs_file_name, ok := fileNameMap.Load(fileID)
	if !ok{
		fmt.Println("Error loading hydfs file name from fileNameMap")
		return
	}
	send_file_name:=  "original_" + fileID + ".txt" +"+" + hydfs_file_name.(string)
	sendHyDFSFile(lc, conn, "original", file_name, send_file_name)
	//send the logical append files 
	//check if there are any logical appends for this file 
	dir := GetDistributedLogQuerierDir()
	appendDir := filepath.Join(dir, "appendBay")
	nodeDirs, err := os.ReadDir(appendDir)
	if err != nil {
		fmt.Println("Error reading append directory:", err)
		return
	}
	//read the nodeDir folders
	for _, nodeDirF := range nodeDirs {
		nodeDir := filepath.Join(appendDir, nodeDirF.Name())
		nodeID := nodeDirF.Name()
		if _, err := os.Stat(nodeDir); os.IsNotExist(err) {
			fmt.Println("No logical appends found for file - NO OP " + fileID)
			return
		}
		files, err := os.ReadDir(nodeDir)
		if err != nil {
			fmt.Println("Error reading append directory:", err)
			return
		}
		//send the logical append files
		for _, file := range files {
				node_file_path:= filepath.Join(nodeID, file.Name())
				fmt.Println("Node file path for append" + node_file_path)
				fmt.Println("Sending logical append file " + file.Name() + " to " + conn.RemoteAddr().String())
				sendHyDFSFile(lc, conn, "append", node_file_path, file.Name())
		}
	}
}

func createFile(lc *LamportClock, local_filename string ,HydfsfileName string,connTable *sync.Map,keyTable *sync.Map,fileNameMap *sync.Map, self_id int, m int){
	//while creating a file, following steps need to be followed
	//1. generate fileID from the filename
	//2. check if the file already exists at the coordinator 
	//3. if it does not then create the file at the coordinator

	//generate the fileID
	fileID := GetFileID(HydfsfileName, m)
	//get the coordinator for this file
	coordinator := GetHyDFSCoordinatorID(keyTable, fileID)
	if coordinator == -1{
		fmt.Println("Not enough keys while checking for COOD")
		return
	}
	fileName:= "original_" + strconv.Itoa(fileID) + ".txt"
	if self_id == coordinator{
		//copy file contents to the file bay
		//first check if file exists in the file bay
		if checkFileExists("FileBay", fileName){
			fmt.Println("File "+ HydfsfileName +" already exists in HYDFS (local)")
			return
		}
		dir:= GetDistributedLogQuerierDir()
		filePath := filepath.Join(dir, "FileBay", fileName)
		//construct the original file path 
		originalFilePath := filepath.Join(dir, "business", local_filename)
		//first create the file
		_, err := os.Create(filePath)
		if err != nil {
			fmt.Println("Error creating file:", err)
			return
		}
		//check if the file exists in the business directory
		if !checkFileExists("business", local_filename){
			fmt.Println("File to be created does not exist in the business directory")
			return
		}
		//copy the file contents to the file bay
		err_copy := copyFileContents(originalFilePath, filePath)
		if err_copy != nil{
			fmt.Println("Error copying file to fileBay")
			return
		}
		//broadcast the fileName to the replicas
		broadcastHyDFSMessage(lc, connTable, "MAPSFILE: " + strconv.Itoa(fileID) + " " + HydfsfileName)
		fileNameMap.Store(strconv.Itoa(fileID), HydfsfileName)
		fmt.Println("File " + fileName + " successfully created in HYDFS (local)")

	}else{
		//use the check exist functionality
		//get the conn for the coordinator
		conn, ok := connTable.Load(coordinator)
		if !ok{
			fmt.Println("Error loading conn from connTable")
			return
		}
		fmt.Println("Sending check exist message to " + conn.(net.Conn).RemoteAddr().String())
		//also send hydfsFilename for recovery purposes
		sendHyDFSMessage(lc, conn.(net.Conn), "CHECKEXIST " + strconv.Itoa(fileID) + " " + local_filename + " " + HydfsfileName)
	}
}

//function to be run only at the coordinator
func LogicalTempMerge(fileID string, requestor_node_id string) string {
	//steps:
	//1. check if the file exists in the file bay
	//2. check if any logical appends exist for this file
	//3. collect the append number and sort them with lamport timestamps for the requestor node ONLY
	//4. merge the file with the logical appends and store it in temp
	dir:= GetDistributedLogQuerierDir()
	temp_dir := filepath.Join(dir, "temp")
	fileBay_dir := filepath.Join(dir, "FileBay")
	filePath := filepath.Join(fileBay_dir, "original_" + fileID)
	filePath = filePath + ".txt"
	//check if the file exists in the file bay
	if !checkFileExists("FileBay", "original_" + fileID){
		fmt.Println("File to be logically merged does not exist in the file bay")
		return ""
	}
	//check if there are any logical appends from the requestor node
	appendDir := filepath.Join(dir, "appendBay", requestor_node_id)	
	if _, err := os.Stat(appendDir); os.IsNotExist(err) {
		fmt.Println("No logical appends found for file - NO OP" + fileID)
		//simply copy the file to the temp dir 
		tempFilePath := filepath.Join(temp_dir, "cache_" + fileID)
		tempFilePath = tempFilePath + ".txt"
		err := copyFileContents(filePath, tempFilePath)
		if err != nil {
			fmt.Println("Error copying file to temp:", err)
			return ""
		}
		return tempFilePath
	}else{
		node_list:= make([]StringIntPair, 0)
		files, err := os.ReadDir(appendDir)
		if err != nil {
			fmt.Println("Error reading append directory from logical temp merge :", err)
			return ""
		}
		fmt.Println("Files found in appendDir", files)
		for _, file := range files {
			//filename like append_434_134_500.txt
			// 434 is nodeId, 134 is fileID, 500 is lamport timestamp
			fileBase:= strings.Split(file.Name(), ".")[0]
			fileID_parse := strings.Split(fileBase, "_")[2] 
			if fileID_parse == fileID{
				//get the lamport timestamp
				fmt.Println(fileBase)
				fmt.Println("Lamport :" + strings.Split(fileBase, "_")[3])
				lamport_timestamp, err := strconv.Atoi(strings.Split(fileBase, "_")[3])
				if err != nil{
					fmt.Println("Error converting lamport string to int")
					return ""
				}
				node_list = append(node_list, StringIntPair{filepath.Join(appendDir, file.Name()), lamport_timestamp})
			}
		}
		//sort the node_list based on the lamport timestamp
		sort.Slice(node_list, func(i, j int) bool {
			return node_list[i].Value < node_list[j].Value
		})
		//merge the file with the logical appends
		//first read the original file
		originalFileData, err := os.ReadFile(filePath)
		if err != nil {
			fmt.Println("Error opening file:", err)
			return ""
		}
		//append in place the logical appends with newline
		fmt.Println("Node list for logical merge", node_list)
		originalFileData = append(originalFileData, []byte("\n")...)
		for _, appendFile := range node_list{
			appendFileData, err := os.ReadFile(appendFile.Key)
			if err != nil {
				fmt.Println("Error opening file:", err)
				return ""
			}
			originalFileData = append(originalFileData, appendFileData...)
			originalFileData = append(originalFileData, []byte("\n")...)
		}
		//write the merged file back to the temp file
		tempFilePath := filepath.Join(temp_dir, "cache_" + fileID)
		tempFilePath = tempFilePath + ".txt"
		err = os.WriteFile(tempFilePath, originalFileData, 0644)
		if err != nil {
			fmt.Println("Error writing file to temp:", err)
			return ""
		}
		fmt.Println("File " + fileID + " successfully logically merged")
		return tempFilePath
	}
}

func getFile(lc *LamportClock, fileID string, connTable *sync.Map, keyTable *sync.Map, self_id int, requestor_node_id int) bool {
	//steps
	//1. check if the file exists in cacheBay
	//2. if it does then show the file to the user
	//3. if it does not then first check if you are the cood for this file
	//4. if you are the cood, create a temp file with all the logical caches and add the file to cacheBay 
	//5. if you are not the cood, send a message to the cood to get the file (also loads the file in cacheBay)
	if requestor_node_id == self_id{
		res :=checkFileExists("CacheBay", "cache_" + fileID + ".txt")
		if res{
			fmt.Println("File "+ fileID +" already exists in locally in cache and is not stale")
			return true
		}
	}
	//get the coordinator for this file
	fileID_int, err := strconv.Atoi(fileID)
	if err != nil{
		fmt.Println("Error converting fileID to int")
		return false
	}
	coordinator := GetHyDFSCoordinatorID(keyTable, fileID_int)
	if coordinator == -1{
		fmt.Println("Not enough keys while checking for COOD")
		return false
	}
	if self_id == coordinator{
		//create the file in the cache bay
		//first check if the file exists in the file bay
		if !checkFileExists("FileBay", "original_" + fileID+".txt"){
			fmt.Println("File to be fetched does not exist at the supposed cood, file not found!")
			if requestor_node_id == self_id{
				fmt.Println("Requested File " + fileID + " does not exist in HYDFS.")
			}
			return false
		}
		//copy the file to the cache bay
		dir := GetDistributedLogQuerierDir()
		cachePath := filepath.Join(dir, "CacheBay", "cache_" + fileID+".txt")
		tmpPath := LogicalTempMerge(fileID, strconv.Itoa(requestor_node_id))
		//send the file to the requestor node
		if requestor_node_id == self_id{
			//copy the file to the cache bay
			err := copyFileContents(tmpPath, cachePath)
			if err != nil {
				fmt.Println("Error copying file to CacheBay:", err)
				return false
			}
			fmt.Println("File " + fileID + " successfully fetched and now can be read")
			//remove the temp file
			removeFile(tmpPath)
			return true
		}else{
			conn, ok := connTable.Load(requestor_node_id)
			if !ok{
				fmt.Println("Error loading conn from connTable")
				return false
			}
			fmt.Println("Sending cache file " + fileID + " to " + conn.(net.Conn).RemoteAddr().String())
			sendHyDFSFile(lc, conn.(net.Conn), "temp", "cache_" + fileID , "cache_" + fileID)
			//remove the temp file
			removeFile(tmpPath)
			return true
		}
	}else{
		//send the get message to the cood
		conn, ok := connTable.Load(coordinator)
		if !ok{
			fmt.Println("Error loading conn from connTable")
			return false
		}
		fmt.Println("Sending get file message to " + conn.(net.Conn).RemoteAddr().String())
		sendHyDFSMessage(lc, conn.(net.Conn), "GETFILE: " + fileID + " " + strconv.Itoa(requestor_node_id))
		return true
	}
	
}

//tries to find fileID in replica Bay or the Filebay, should be only called on the cood
func mergeFile(fileID string){
	//merge the file on this node
	//steps to merge the file
	//1. check if the file exists in the file bay
	//2. check if any logical appends exist for this file
	//3. collect the append number and sort them with lamport timestamps PER client
	//4. merge the file with the logical appends
	//5. remove the logical appends

	//check if the file exists in the file bay
	if !checkFileExists("FileBay", "original_" + fileID+".txt") && !checkFileExists("ReplicaBay", "replica_" + fileID+".txt"){
		fmt.Println("File to be merged does not exist in the file bay or replica bay")
		return
	}
	//check if the file exists in the file bay OR the replica bay
	//get the file path
	dir := GetDistributedLogQuerierDir()
	filePath := filepath.Join(dir, "FileBay", "original_" + fileID)
	if checkFileExists("FileBay", "original_" + fileID){
		filePath = filepath.Join(dir, "FileBay", "original_" + fileID + ".txt")
	}else if checkFileExists("ReplicaBay", "replica_" + fileID){
		filePath = filepath.Join(dir, "ReplicaBay", "replica_" + fileID + ".txt")
	}
	//iterate over the appendsBay to search the logical appends for this file
	// the appendbay dir is structured as appendBay/nodeID/append_file

	appendDir := filepath.Join(dir, "appendBay")
	nodeDirs, err := os.ReadDir(appendDir)
	if err != nil {
		fmt.Println("Error reading append directory:", err)
		return
	}

	//use a queue to store the logical appends
	queue:= make([]string, 0)
	type StringIntPair struct {
		Key   string
		Value int
	}
	
	for _, nodeDir := range nodeDirs {
		//walk the nodeDir, if the file name contains the fileID, push the file path and int of lamport
		//timestamp to the node_list
		node_list:= make([]StringIntPair, 0)
		files, err := os.ReadDir(filepath.Join(appendDir, nodeDir.Name()))
		if err != nil {
			fmt.Println("Error reading append directory:", err)
			return
		}
		for _, file := range files {
			//filename like append_434_134_500.txt
			// 434 is nodeId, 134 is fileID, 500 is lamport timestamp
			fileID_parse := strings.Split(file.Name(), "_")[2] 
			if fileID_parse == fileID{
				//get the lamport timestamp
				fileBase := strings.Split(file.Name(), ".")[0]
				lamport_timestamp, err := strconv.Atoi(strings.Split(fileBase, "_")[3])
				if err != nil{
					fmt.Println("Error converting lamport string to int")
					return
				}
				node_list = append(node_list, StringIntPair{filepath.Join(appendDir, nodeDir.Name(), file.Name()), lamport_timestamp})
			}
		}
		//sort the node_list based on the lamport timestamp
		sort.Slice(node_list, func(i, j int) bool {
			return node_list[i].Value < node_list[j].Value
		})
		//push the file paths to the queue
		for _, pair := range node_list{
			queue = append(queue, pair.Key)
		}
	}
	if len(queue) == 0{
		fmt.Println("No logical appends found for file - NO OP" + fileID)
		return
	}
	//merge the file with the logical appends
	//first read the original file
	originalFileData, err := os.ReadFile(filePath)
	if err != nil {
		fmt.Println("Error opening file:", err)
		return
	}
	//append in place the logical appends with newline 
	originalFileData = append(originalFileData, []byte("\n")...)
	for _, appendFile := range queue{
		appendFileData, err := os.ReadFile(appendFile)
		if err != nil {
			fmt.Println("Error opening file:", err)
			return
		}
		originalFileData = append(originalFileData, appendFileData...)
		originalFileData = append(originalFileData, []byte("\n")...)
	}
	//write the merged file back to the original file
	err = os.WriteFile(filePath, originalFileData, 0644)
	if err != nil {
		fmt.Println("Error writing file:", err)
		return
	}
	//remove the logical appends
	for _, appendFile := range queue{
		removeFile(appendFile)
	}
	fmt.Println("File " + fileID + " successfully merged")
}

func removeFile(filePath string){
	//check if the file exists
	if _, err := os.Stat(filePath); os.IsNotExist(err) {
		if strings.Contains(filePath, "cache"){
			//silent remove trial for cache files
			return 
		}
		fmt.Println("File does not exist for removal " + filePath)
		return
	}
	err := os.Remove(filePath)
	if err != nil {
		fmt.Println("Error removing file:", err)
	}
}
//only run this function if the append operation is called on the cood for that fileID
//does NOT call forward append to replica
func appendLocal(lc *LamportClock,local_filename string, toAppendFileName string, m int )string{
	//check if the fileID exists in the file bay
	fileID := GetFileID(toAppendFileName,m)
	if !checkFileExists("FileBay", "original_" + strconv.Itoa(fileID)){
		fmt.Println("File to append to does not exist in the file bay")
		return ""
	}
	//copy the contents of the file to the append bay
	dir := GetDistributedLogQuerierDir()
	src_path := filepath.Join(dir, "business", local_filename)
	append_file_name := "append_" + strconv.Itoa(getSelf_id())+"_"+strconv.Itoa(fileID) + "_" + strconv.Itoa(int(lc.timestamp)) + ".txt"
	//check if the directory exists
	appendBayDir := filepath.Join(dir, "appendBay", strconv.Itoa(getSelf_id()))
	if _, err := os.Stat(appendBayDir); os.IsNotExist(err) {
		os.Mkdir(appendBayDir, 0755)
	}
	dest_path := filepath.Join(appendBayDir, append_file_name)
	err := copyFileContents(src_path, dest_path)
	if err != nil {
		fmt.Println("Error copying file to append bay (local):", err)
		return ""
	}
	fmt.Println("File " + local_filename + " successfully appended to " + toAppendFileName)
	return dest_path
}

func appendFile(lc *LamportClock, connTable *sync.Map, keyTable *sync.Map, self_id int, local_filename string, toAppendFileName string, m int){
	//check if the fileID exists in the file bay
	fileID := GetFileID(toAppendFileName,m)
	coordinator := GetHyDFSCoordinatorID(keyTable, fileID)
	if coordinator == -1{
		fmt.Println("Not enough keys while checking for COOD")
		return
	}
	if self_id == coordinator{
		//append the file locally
		appendPath := appendLocal(lc, local_filename, toAppendFileName, m)
		if appendPath == ""{
			fmt.Println("Error appending file locally")
			return	
		}
		//fetch only the filename from the appendPath
		appendFileName := filepath.Base(appendPath)
		//forward the append to the replicas
		forwardAppendToReplica(lc, connTable, keyTable, self_id,appendPath, appendFileName)
	}else{
		//send the append message to the coordinator
		conn, ok := connTable.Load(coordinator)
		if !ok{
			fmt.Println("Error loading conn from connTable")
			return
		}
		//fmt.Println("Sending append file to cood : " + conn.(net.Conn).RemoteAddr().String())
		append_file_name := "append_" + strconv.Itoa(getSelf_id())+"_"+strconv.Itoa(fileID) + "_" + strconv.Itoa(int(lc.timestamp)) + ".txt"
		sendHyDFSFile(lc, conn.(net.Conn), "business", local_filename, append_file_name)
	}
	//invalidate the cache for this file i.e. remove file from cacheBay
	removeFile(filepath.Join(GetDistributedLogQuerierDir(), "CacheBay", "cache_" + strconv.Itoa(fileID)+".txt"))
}
//requires actual filename, not the fileID
func checkFileExists(dir_name string, file_name string) bool{
	if !strings.Contains(file_name, ".txt"){
		file_name = file_name + ".txt"
	}
	dir := GetDistributedLogQuerierDir()
	filePath := filepath.Join(dir, dir_name, file_name)
	//fmt.Println("Checking if file exists " + filePath)
	if _, err := os.Stat(filePath); os.IsNotExist(err) {
		//fmt.Println("File does not exist " + file_name + " in " + dir_name)
		return false
	}
	return true
}
func displayKeyTable(keyTable *sync.Map){
	keys := make([]int, 0)
	fmt.Println("HyDFS Nodes")
	keyTable.Range(func(k, v interface{}) bool {
		keys = append(keys, k.(int))
		fmt.Println(k, v)
		return true
	})
	//also show sorted keys 
	sort.Ints(keys)
	fmt.Println("Sorted Keys : ")
	for _, key := range keys{
		fmt.Print(strconv.Itoa(key) + " ")
	}
	fmt.Println("\n*****")
		
}

func sendHyDFSMessage(lc *LamportClock,conn net.Conn, msg string){
	conn.Write([]byte(msg + "END_OF_MSG\n"))
	lc.Increment()
}

func broadcastHyDFSMessage(lc *LamportClock,connTable *sync.Map, msg string){
	connTable.Range(func(key, value interface{}) bool {
		//do not send the message to the current node
		if key == self_id{
			return true
		}
		conn := value.(net.Conn)
		sendHyDFSMessage(lc, conn, msg)
		return true
	})
}

//file type can be one of replica, append, main, fileType is for source 
func sendHyDFSFile(lc *LamportClock, conn net.Conn, fileType string, local_filename string, sendFileName string){
	dir := GetDistributedLogQuerierDir()
	var filePath string
	if !strings.Contains(local_filename, ".txt"){
		local_filename = local_filename + ".txt"
	}
	if !strings.Contains(sendFileName, ".txt"){
		sendFileName = sendFileName + ".txt"
	}
	if fileType == "replica"{
		//send from the replica bay
		filePath = filepath.Join(dir, "ReplicaBay", local_filename)
	}else if fileType == "append"{
		//send from the append bay
		//filename must contain the nodeID dir when the type is append!
		//fmt.Println("Local Filename for append " + local_filename)
		filePath = filepath.Join(dir, "appendBay",local_filename)
		//fmt.Println("Append Bay constructed path " + filePath)
	}else if fileType == "original"{
		//send from the file bay
		filePath = filepath.Join(dir, "FileBay", local_filename)
	}else if fileType=="business"{
		//send from the business dir
		filePath = filepath.Join(dir, "business", local_filename)
	}else if fileType=="temp"{
		filePath = filepath.Join(dir, "temp", local_filename)
	}else {
		fmt.Println("Invalid file type found in sendHyDFSFile")
		return
	}
	//fmt.Println("Sending file " + local_filename + " to " + conn.RemoteAddr().String())
	fileData, err := os.ReadFile(filePath)
    if err != nil {
        fmt.Println("Error opening file:", err)
		return
    }
    // Create message with header
    header := fmt.Sprintf("FILE: %s %s\n", fileType, sendFileName)
    message := append([]byte(header), fileData...)
	//add end of message
	message = append(message, []byte("\nEND_OF_MSG\n")...)
    // Send the entire message
    _, errW := conn.Write(message)
    if errW != nil {
        fmt.Println("Error sending file:", errW, sendFileName, local_filename)
    }
	lc.Increment()
    fmt.Printf("File %s sent successfully\n", filepath.Base(filePath))
}

func receiveHyDFSFile(lc *LamportClock, message string) {
    lines := strings.SplitN(message, "\n", 2)
    if len(lines) < 2 {
       fmt.Println("Invalid file message format")
	   return 
    }
    // Parse the header
    header := strings.Fields(lines[0])
    if len(header) < 3 {
        fmt.Println("Invalid file message header")
    }
    fileName := header[2]
	//fmt.Println("Received file " + fileName)
    // Save the file data
    fileData := []byte(lines[1])
	dir := GetDistributedLogQuerierDir()
	destPath := filepath.Join(dir, "ArrivalBay", fileName)
    if err := os.WriteFile(destPath, fileData, 0644); err != nil {
        fmt.Println("Error saving file:", err)
    }

    lc.Increment()
    //fmt.Printf("File %s received and saved\n", fileName)
}

func forwardAppendToReplica(lc *LamportClock, connTable *sync.Map, keyTable *sync.Map ,self_id int , filePath string, fileName string){
	//get the successor to the current node
	suc_1, suc_2 := GetHYDFSSuccessorIDs(self_id, keyTable)
	//get conns for these successors
	conn1, ok1 := connTable.Load(suc_1)
	conn2, ok2 := connTable.Load(suc_2)
	if !ok1 || !ok2{
		fmt.Println("Error loading conn from connTable")
		return
	}
	//fmt.Println("Forwarding append file " + fileName + " to " + conn1.(net.Conn).RemoteAddr().String() + filePath)
	//fmt.Println("Forwarding append file " + fileName + " to " + conn2.(net.Conn).RemoteAddr().String() + filePath)

	//send the file to the successors
	//must send append/29/fileName as local_filename
	node_ID := strings.Split(fileName, "_")[1]
	//fmt.Println("Node ID for append " + node_ID)
	node_file_path:= filepath.Join(node_ID, fileName)
	//fmt.Println("Node file path for append " + node_file_path)
	sendHyDFSFile(lc, conn1.(net.Conn), "append", node_file_path, fileName)
	sendHyDFSFile(lc, conn2.(net.Conn), "append", node_file_path, fileName)
	//fmt.Println("Append File " + fileName + " successfully forwarded to successors")
}

//TODO: optimize the keyTableAdd and keyTableRemove functions, store a readonly sorted list of keys in keyTable with key -1 or something
func GetHyDFSCoordinatorID(keyTable * sync.Map, fileID int) int {
	//search key table for the right co-ordinator
	//get the keys from the table and then sort them
	keys := make([]int, 0)
	keyTable.Range(func(k, v interface{}) bool {
		keys = append(keys, k.(int))
		return true
	})
	//sort the keys
	sort.Ints(keys)
	//find the position of the key in the sorted list
	index := sort.Search(len(keys), func(i int) bool {
		return keys[i] >= fileID
	})
	if index == len(keys){
		if len(keys) == 0{
			return -1
		}
		return keys[0]
	}
	return keys[index]
}

//used by stage1 to map the hash to the node
func MapHashableToNodeAndTask(word string, m int, nodeIDs []int, taskIDs [] int) (int,int) {
	//first find word hash
	fmt.Println("Word to be hashed " + word)
	fmt.Println(nodeIDs)
	fmt.Println(taskIDs)
	word_ID := GetFileID(word, m)
	//find the nodeID
	index:= word_ID % len(nodeIDs)
	nodeID := nodeIDs[index]
	taskID := taskIDs[index]
	//fmt.Println("WI", word, index)
	return nodeID, taskID

}

// logical extension of new co-ordinators etc
func KeyTableAdd(keyTable *sync.Map, key int){
	
	//special cases for len 0 and 1 
	//for 0 
	length := getSyncMapLength(keyTable)
	if length == 0{
		value := []int{key,key}
		keyTable.Store(key, value)
		return
	}else if length == 1{
		//for 1
		//get the only key in the table
		var key1 int
		keyTable.Range(func(k, v interface{}) bool {
			key1 = k.(int)
			return false
		})
		if key < key1{
			keyTable.Store(key1, []int{key1,key})
			keyTable.Store(key, []int{key,key1})
		}else{
			keyTable.Store(key1, []int{key,key1})
			keyTable.Store(key, []int{key1,key})
		}
	}else{
		//for len keyTable more than 1
		//get the keys from the table and then sort them 
		keys := make([]int, 0)
		keyTable.Range(func(k, v interface{}) bool {
			keys = append(keys, k.(int))
			return true
		})
		//sort the keys
		sort.Ints(keys)
		//find the position of the key in the sorted list
		index := sort.Search(len(keys), func(i int) bool {
			return keys[i] >= key
		})
		prev_key, nextkey := -1, -1
		
		if index == 0{
			prev_key = keys[len(keys)-1]
			nextkey = keys[0]
		}
		if index == len(keys){
			prev_key = keys[len(keys)-1]
			nextkey = keys[0]
		}
		if index > 0 && index < len(keys){
			prev_key = keys[index-1]
			nextkey = keys[index]
		}
		

		//update the keyTable
		keyTable.Store(nextkey, []int{key,nextkey})
		keyTable.Store(key, []int{prev_key,key})
	}
}

// logical shrinking of the key table
func KeyTableRemove(keyTable *sync.Map, key int){
	lenKT := getSyncMapLength(keyTable)
	if lenKT == 0{
		return
	}else if lenKT == 1{
		keyTable.Delete(key)
	}else{
		//first get the value for that key 
		val,ok := keyTable.Load(key)
		if !ok{
			fmt.Println("Error loading key from keyTable")
			return
		}
		intSlice,ok  := val.([]int)
		if !ok{
			fmt.Println("Error converting interface to int slice")
			return
		}
		//find the next_key to this key
		keys := make([]int, 0)
		keyTable.Range(func(k, v interface{}) bool {
			keys = append(keys, k.(int))
			return true
		})
		//sort the keys
		sort.Ints(keys)
		//find the position of the key in the sorted list
		index := sort.Search(len(keys), func(i int) bool {
			return keys[i] >= key
		})
	
		// Check if the found index actually contains the target value
		if index >= len(keys) || keys[index] != key {
			return 
		}
		//get the next key
		next_key := -1
		if index == len(keys)-1{
			next_key = keys[0]
		}else{
			next_key = keys[index+1]
		}
		stitch_key := intSlice[0]
		keyTable.Store(next_key, []int{stitch_key, next_key})
		keyTable.Delete(key)
	}
}

func ConnTableAdd(connTable *sync.Map, key int, conn net.Conn){
	 connTable.Store(key, conn)
}
func ConnTableRemove(connTable *sync.Map, key int){
	connTable.Delete(key)
}

// getPeerID generates a peer ID by hashing the IP address and port,
// then truncating the result to m bits.
func GetPeerID(ipAddress string, m int) int {
	identifier := ipAddress
	
	// Hash the identifier using SHA-1
	hash := sha1.Sum([]byte(identifier))
	
	// Convert the first m bits of the hash to an integer
	bitLength := m / 8
	if m%8 != 0 {
		bitLength += 1
	}
	
	// Truncate the hash to the required number of bits
	hashTruncated := hash[:bitLength]

	// Convert truncated hash to big.Int and shift to ensure only m bits are used
	bigIntHash := new(big.Int).SetBytes(hashTruncated)
	bigIntMod := new(big.Int).Lsh(big.NewInt(1), uint(m))
	peerID := new(big.Int).Mod(bigIntHash, bigIntMod)

	return int(peerID.Int64())
}
// getFileID generates a file ID by hashing the filename and truncating the result to m bits.
func GetFileID(filename string, m int) int {
	// Hash the filename using SHA-1
	hash := sha1.Sum([]byte(filename))
	
	// Calculate the number of bytes needed to cover m bits
	bitLength := m / 8
	if m%8 != 0 {
		bitLength += 1
	}

	// Truncate the hash to the required number of bits
	hashTruncated := hash[:bitLength]

	// Convert truncated hash to big.Int and apply modulo 2^m
	bigIntHash := new(big.Int).SetBytes(hashTruncated)
	bigIntMod := new(big.Int).Lsh(big.NewInt(1), uint(m))
	fileID := new(big.Int).Mod(bigIntHash, bigIntMod)
	//fmt.Println("FileID: for  " + filename + " is ", int(fileID.Int64()))
	return int(fileID.Int64())
}

func GetAddressfromHash(hash *string) string {
	tokens := strings.Split(*hash, "-")
	return tokens[0]
}

func mapToNextMultipleOf9(num int) int {
    return ((num / 9) + 1) * 9
}

func GetOperatorsDirLocal(taskID int) string{
	base_lim := mapToNextMultipleOf9(taskID)
	standardizer :=  base_lim - 9 
	taskID -= standardizer
	_,currentFile,_,_ := runtime.Caller(0)
	dir := filepath.Dir(currentFile) //core_process
	dir  = filepath.Dir(dir) //distributed_log_querier
	dir = filepath.Join(dir, "operators")
	dir = filepath.Join(dir, "operators_parallel")
	dir = filepath.Join(dir, "operator" + strconv.Itoa(taskID))
	return dir
}

// func GetOperatorsDir() string{
// 	_,currentFile,_,_ := runtime.Caller(0)
// 	dir := filepath.Dir(currentFile) //core_process
// 	dir  = filepath.Dir(dir) //distributed_log_querier
// 	dir = filepath.Join(dir, "operators")
// 	return dir
// }

func GetDistributedLogQuerierDir() string{
	//for local testing call self_id 
	_,currentFile,_,_ := runtime.Caller(0)
	dir := filepath.Dir(currentFile) //core_process
	dir  = filepath.Dir(dir) //distributed_log_querier
	dir = filepath.Dir(dir) // G28 
	dir = filepath.Join(dir, "HYDFS") // G28/HYDFS
	//fmt.Println("Directory for HYDFS original " + dir)
	//VM MARKER
	// self_id := getSelf_id()
	// dir = filepath.Join(dir, "Nuke")
	// dir = filepath.Join(dir, "Node" + strconv.Itoa(self_id))
	//END VM MARKER
	//create the directory if it does not exist
	for _, dirSub := range []string{"FileBay", "appendBay", "ReplicaBay", "CacheBay", "ArrivalBay", "business", "temp", "Fetched"}{
		if _, err := os.Stat(filepath.Join(dir, dirSub)); os.IsNotExist(err) {
			os.Mkdir(filepath.Join(dir, dirSub), 0755)
			//fmt.Println("Directory created at " + filepath.Join(dir, dirSub))
		}
	}
	if _, err := os.Stat(dir); os.IsNotExist(err) {
		os.Mkdir(dir, 0755)
		fmt.Println("Directory created at " + dir)
		//create the file bay, append bay, replica bay, cache bay
		os.Mkdir(filepath.Join(dir, "FileBay"), 0755)
		os.Mkdir(filepath.Join(dir, "appendBay"), 0755)
		os.Mkdir(filepath.Join(dir, "ReplicaBay"), 0755)
		os.Mkdir(filepath.Join(dir, "CacheBay"), 0755)
		os.Mkdir(filepath.Join(dir, "ArrivalBay"), 0755)
		os.Mkdir(filepath.Join(dir, "business"), 0755)
		os.Mkdir(filepath.Join(dir, "temp"), 0755)
		os.Mkdir(filepath.Join(dir, "Fetched"), 0755)
	}
	//create an empty.txt file by default in the business directory
	emptyFile := filepath.Join(dir, "business", "empty.txt")
	if _, err := os.Stat(emptyFile); os.IsNotExist(err) {
		os.Create(emptyFile)
		//write "EMPTYFILE:" to the empty file
		err := os.WriteFile(emptyFile, []byte("EMPTYFILE: \n"), 0644)
		if err != nil {
			fmt.Println("Error writing file:", err)
		}
		}
	
	return dir
}

func deleteDirectoriesExceptBusiness(dirPath string) error {
    entries, err := os.ReadDir(dirPath)
    if err != nil {
        return fmt.Errorf("failed to read directory: %v", err)
    }
    for _, entry := range entries {
        if entry.IsDir() && entry.Name() != "business" {
            fullPath := filepath.Join(dirPath, entry.Name())
            err := os.RemoveAll(fullPath)
            if err != nil {
                return fmt.Errorf("failed to remove directory %s: %v", fullPath, err)
            }
            fmt.Printf("Deleted directory: %s\n", fullPath)
        }
    }

    return nil
}

// Helper function to copy file contents
func copyFileContents(src, dst string) error {
	if !strings.Contains(src, ".txt"){
		src = src + ".txt"
	}
	if !strings.Contains(dst, ".txt"){
		dst = dst + ".txt"
	}
    sourceFile, err := os.Open(src)
	if err != nil {
		return err
	}
    defer sourceFile.Close()
    destFile, err := os.Create(dst)
    if err != nil {
        return err
    }
    defer destFile.Close()

    _, err = io.Copy(destFile, sourceFile)
	//fmt.Println("Copy Done")
    return err
}

//function to be only called from failure detection code (MP2) or from StreamDS (MP4)
func sendUpdateToHYDFS(hydfsConn *SafeConn, message string){
	_, err := hydfsConn.SafeWrite([]byte(message + "END_OF_MSG\n"))
	if err != nil {
		fmt.Println("Error sending message to HYDFS:", err)
	}
}

//node hash like address:port-incarnation
func convertNodeHashForHYDFS(nodeHash string) string {
	token := strings.Split(nodeHash, "-")[0]
	//VM MARKER
	// port := strings.Split(token, ":")[1]
	// address:= strings.Split(token, ":")[0]
	// a_int, err :=strconv.Atoi(port)
	// if err != nil {
	// 	fmt.Println("Error in converting string to int in port")
	// 	return ""
	// }
	// a_int = a_int - 2020
	// return address + ":" + strconv.Itoa(a_int)
	//END VM MARKER
	return token 
	
}

func convertNodeHashForStreamDS(nodeHash string) string{	
	return nodeHash
}

//function expects the Sync.Map to be key (nodehash):string, value (status): string 
func AddToMembershipList(membershipList *sync.Map, nodeHash string, incarnationNum int, hydfsConn *SafeConn) {
	string_val := "ALIVE" + "$" + strconv.Itoa(incarnationNum)
	//type  10.193.209.248:8081-1.0
	membershipList.Store(nodeHash, string_val)
	//send update to hydfs layer
	
	sendUpdateToHYDFS(hydfsConn, "ADD " + convertNodeHashForHYDFS(nodeHash))
}
func AddToMembershipListWithStatus(membershipList *sync.Map, nodeHash string, status string, incarnationNum int, hydfsConn *SafeConn) {
	string_val := status + "$" + strconv.Itoa(incarnationNum)
	fmt.Println("Adding to membership list ", string_val)
	membershipList.Store(nodeHash, string_val)
	//send update to hydfs layer
	sendUpdateToHYDFS(hydfsConn, "ADD " + convertNodeHashForHYDFS(nodeHash))
}

func DeleteFromMembershipList(membershipList *sync.Map, nodeHash string, hydfsConn *SafeConn) {
	membershipList.Delete(nodeHash)
	//send update to hydfs layer
	sendUpdateToHYDFS(hydfsConn, "REMOVE " + convertNodeHashForHYDFS(nodeHash))
}
//used for Suspicion mechanism 
func UpdateMembershipList(membershipList *sync.Map, nodeHash string ,status string, incarnationNum int) {
	membershipList.Store(nodeHash, status+"$"+strconv.Itoa(incarnationNum))
}
//returns -1 if node is not in the membership list
func GetIncarnationNum(membershipList *sync.Map, nodeHash string) int {
	value, ok := membershipList.Load(nodeHash)
	if ok {
		//fmt.Println("Value ", value)
		tokens := strings.Split(value.(string), "$")
		//fmt.Println("Tokens", tokens)
		incarnationNum := tokens[1]
		num, err:= strconv.Atoi(incarnationNum)
		if err!=nil{
			fmt.Println("Error converting string to int 50")
			return -1
		}
		return num
	}else {
		return -1
	}
}

func subtractStrings(a string, b int) string{
	//convert a to int and the subtract b from it
	//return the result as string
	a_int, err :=strconv.Atoi(a)
	if err != nil {
		fmt.Println("Error in converting string to int in subtract strings")
		return ""
	}
	a_int = a_int - b
	return strconv.Itoa(a_int)
}

func GetStatus(membershipList *sync.Map, nodeHash string) string {
	value, ok := membershipList.Load(nodeHash)
	if ok {
		tokens := strings.Split(value.(string), "$")
		return tokens[0]
	}else {
		return "DEAD"
	}
}

func SetIncarnationNum(membershipList *sync.Map, nodeHash string, incarnationNum int) {
	value, ok := membershipList.Load(nodeHash)
	if ok {
		tokens := strings.Split(value.(string), "$")
		status := tokens[0]
		membershipList.Store(nodeHash, status+"$"+strconv.Itoa(incarnationNum))
	}
}



//returns a list of all the nodes in the membership list.
//in the format [nodehash1$STATUS,nodehash2$STATUS,...]
func GetMembershipList(membershipList *sync.Map) []string {
	nodes := make([]string, 0)
	membershipList.Range(func(key, value interface{}) bool {
		// Ensure key and value are of the expected type
		k, ok1 := key.(string)
		v, ok2 := value.(string)
		if ok1 && ok2{
			// If both key and value are strings, append the value (or key) to the nodes list
			nodes = append(nodes, k+"$"+v) // or append(nodes, k) if you want the keys
		}
		return true
	})
	return nodes
}
//recalculates the subset list based on the current membership list
func GetRandomizedPingTargets(membershipList *sync.Map,self_hash string) ([]string,[] string) {
	nodes := make([]string, 0)
	membershipList.Range(func(key, value interface{}) bool {
		// Ensure key and value are of the expected type
		key1, ok1 := key.(string)
		//avoid pinging yourself
		if ok1 && key1!=self_hash{
			nodes = append(nodes, key1)
		}
		return true
	})

	//fmt.Println("Random", nodes)
	//random shuffle the list
		// Randomly shuffle the slice
		rand.Shuffle(len(nodes), func(i, j int) {
			nodes[i], nodes[j] = nodes[j], nodes[i]
		})
		// Print the randomly shuffled list
	//fmt.Println("Randomly shuffled list:", nodes)
	totalNodes:= len(nodes) + 1 
	//if there are less than 3 nodes, return the list as is
	if totalNodes<=3{
		addressList:= make([]string, 0)
		for _, node := range nodes {
			addressList = append(addressList, GetAddressfromHash(&node))
		}
		return addressList, nodes
	}
	k := (totalNodes*2)/3
	if(totalNodes<=5){
		k = totalNodes - 1
	}
	fmt.Println("K -> ", k)
	addressList:= make([]string, 0)
	for _, node := range nodes[:k] {
		addressList = append(addressList, GetAddressfromHash(&node))
	}
	return addressList, nodes
}

//used for round robin mechanism
func RandomizeList(nodes []string) []string {
	// Randomly shuffle the slice
	rand.Shuffle(len(nodes), func(i, j int) {
		nodes[i], nodes[j] = nodes[j], nodes[i]
	})
	return nodes
}


func GetMembershipListAddresses(membershipList *sync.Map) []string {
	nodes := make([]string, 0)
	membershipList.Range(func(key, value interface{}) bool {
		// Ensure key and value are of the expected type
		k, ok1 := key.(string)
		if ok1  {
			address:= GetAddressfromHash(&k)
			nodes = append(nodes, address)
		}
		return true
	})
	return nodes
}

func WriteLog(logFileName string, message string) {
	// Get the current file's directory and move up to the parent directory
	_, currentFile, _, _ := runtime.Caller(0)
	dir := filepath.Dir(currentFile)
	dir = filepath.Dir(dir)

	// Construct the full log file path
	fileName := filepath.Join(dir, logFileName)

	// Open the file for appending, create it if it doesn't exist, and write only mode
	file, err := os.OpenFile(fileName, os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		fmt.Println("Error opening or creating log file:", err)
		return
	}
	defer file.Close()

	// Get the current time and prepend it to the message
	currentTime := time.Now().String()
	message = currentTime + " " + message + "\n"

	// Write the message to the file
	_, err_2 := file.WriteString(message)
	if err_2 != nil {
		fmt.Println("Error writing to log file:", err_2)
	}
}

func GetOutboundIP() net.IP {
    conn, err := net.Dial("udp", "8.8.8.8:80")
    if err != nil {
       fmt.Println(err)
    }
    defer conn.Close()

    localAddr := conn.LocalAddr().(*net.UDPAddr)

    return localAddr.IP
}
